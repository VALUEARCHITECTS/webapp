name: Daily Database Backup

on:
  schedule:
    # æ¯Žæ—¥æ·±å¤œ2:00 JSTï¼ˆUTC 17:00å‰æ—¥ï¼‰ã«å®Ÿè¡Œ
    - cron: '0 17 * * *'
  
  # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: main
          fetch-depth: 0
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install Wrangler
        run: npm install -g wrangler
      
      - name: Create backup directory
        run: mkdir -p backups
      
      - name: Export database (READ ONLY)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          echo "ðŸ” Starting database backup (READ ONLY operation)..."
          
          BACKUP_DATE=$(TZ=Asia/Tokyo date +%Y-%m-%d)
          BACKUP_FILE="backups/${BACKUP_DATE}.sql"
          
          echo "ðŸ“… Backup date: ${BACKUP_DATE}"
          echo "ðŸ“ Backup file: ${BACKUP_FILE}"
          
          wrangler d1 export webapp-production --remote --output="${BACKUP_FILE}"
          
          BACKUP_SIZE=$(ls -lh "${BACKUP_FILE}" | awk '{print $5}')
          echo "âœ… Backup completed: ${BACKUP_SIZE}"
          
          echo "ðŸ“„ Backup file preview (first 10 lines):"
          head -10 "${BACKUP_FILE}"
      
      - name: Compress backup
        run: |
          BACKUP_DATE=$(TZ=Asia/Tokyo date +%Y-%m-%d)
          BACKUP_FILE="backups/${BACKUP_DATE}.sql"
          rm -f "${BACKUP_FILE}.gz"
          gzip -k "${BACKUP_FILE}"
          ORIGINAL_SIZE=$(ls -lh "${BACKUP_FILE}" | awk '{print $5}')
          COMPRESSED_SIZE=$(ls -lh "${BACKUP_FILE}.gz" | awk '{print $5}')
          echo "Original size: ${ORIGINAL_SIZE}"
          echo "Compressed size: ${COMPRESSED_SIZE}"
      
      - name: Backup R2 bucket metadata (READ ONLY)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          echo "ðŸ—‚ï¸ Starting R2 bucket metadata backup..."
          BACKUP_DATE=$(TZ=Asia/Tokyo date +%Y-%m-%d)
          R2_BACKUP_DIR="backups/r2-${BACKUP_DATE}"
          mkdir -p "${R2_BACKUP_DIR}"
          
          echo "ðŸ“‹ Fetching file list from database..."
          wrangler d1 execute webapp-production --remote --command="SELECT id, application_type, attachment_url, created_at FROM applications WHERE attachment_url IS NOT NULL ORDER BY created_at DESC;" --json > "${R2_BACKUP_DIR}/file_list.json"
          
          echo "ðŸ“ Extracting file URLs..."
          cat "${R2_BACKUP_DIR}/file_list.json" | grep -o '"attachment_url"[^"]*"[^"]*"' | cut -d'"' -f4 | sort | uniq > "${R2_BACKUP_DIR}/file_urls.txt"
          
          FILE_COUNT=$(cat "${R2_BACKUP_DIR}/file_urls.txt" | wc -l)
          echo "âœ… Total files: ${FILE_COUNT}"
          
          cat > "${R2_BACKUP_DIR}/README.md" << 'EOF'
# R2 Bucket Backup Metadata

This directory contains metadata about files stored in Cloudflare R2 bucket.

## Files

- `file_list.json`: Complete database records with attachment URLs
- `file_urls.txt`: List of all file URLs (one per line)
- `README.md`: This file

## Note

This backup contains file URLs and metadata only.
The actual image files are stored in Cloudflare R2 bucket: `webapp-receipts`

## Cloudflare R2 Protection

Cloudflare R2 has built-in durability (99.999999999%).
Your files are automatically replicated across multiple data centers.

For additional protection:
1. Enable R2 versioning (Cloudflare Dashboard â†’ R2 â†’ Settings)
2. Use R2 lifecycle policies
EOF
          
          echo "âœ… R2 metadata backup completed"
      
      - name: Compress R2 backup
        run: |
          BACKUP_DATE=$(TZ=Asia/Tokyo date +%Y-%m-%d)
          R2_BACKUP_DIR="backups/r2-${BACKUP_DATE}"
          
          if [ -d "${R2_BACKUP_DIR}" ]; then
            tar -czf "${R2_BACKUP_DIR}.tar.gz" -C backups "r2-${BACKUP_DATE}"
            rm -rf "${R2_BACKUP_DIR}"
            
            R2_SIZE=$(ls -lh "${R2_BACKUP_DIR}.tar.gz" | awk '{print $5}')
            echo "âœ… R2 backup compressed: ${R2_SIZE}"
          else
            echo "âš ï¸ R2 backup directory not found, skipping compression"
          fi
      
      - name: Configure Git
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
      
      - name: Fetch and pull latest changes
        run: |
          git fetch origin main
          git pull --rebase origin main || git pull --no-rebase origin main || true
      
      - name: Commit and push backup
        run: |
          BACKUP_DATE=$(TZ=Asia/Tokyo date +%Y-%m-%d)
          
          git add backups/
          
          if git diff --staged --quiet; then
            echo "â„¹ï¸ No changes to commit (backup file already exists)"
          else
            git commit -m "backup: Daily backup for ${BACKUP_DATE} (DB + R2)"
            git push origin main || (git pull --rebase origin main && git push origin main)
            echo "âœ… Backup committed and pushed to GitHub"
          fi
      
      - name: Backup summary
        if: always()
        run: |
          echo "================================================"
          echo "ðŸ“Š Database & R2 Backup Summary"
          echo "================================================"
          echo "ðŸ“… Date: $(TZ=Asia/Tokyo date '+%Y-%m-%d %H:%M:%S JST')"
          echo "ðŸ’¾ Database: webapp-production"
          echo "ðŸ“¦ R2 Bucket: webapp-receipts"
          echo "ðŸ”’ Operation: READ ONLY (no data modified)"
          echo "âœ… Status: ${{ job.status }}"
          echo ""
          echo "ðŸ“ Backup files:"
          ls -lh backups/ | tail -10 || echo "Backups directory not found"
          echo ""
          echo "âœ… All backups are safely stored in GitHub"
          echo "================================================"
